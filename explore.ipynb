{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "#!pip install openai -U\n",
    "from openai import OpenAI\n",
    "from enum import Enum\n",
    "from typing import Optional, List, Tuple\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../key/key.json') as f:\n",
    "    k = json.load(f)['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of 50 human emotions\n",
    "def get_emotions_list():\n",
    "    client = OpenAI(api_key=k)\n",
    "    class Chr_answer(BaseModel):\n",
    "        Characteristics: List[str] = Field(None, description=\"Emotions as a list of strings\")\n",
    "\n",
    "    #few shot prompt with api\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Be a helpful assistant.\"},\n",
    "            {\"role\": \"system\", \"content\": \"Find 50 non-redundant human emotions. These emotions should be different from one another, meaning that for example, of the emotions 'joy' and 'happiness', only pick one. Of 'Shame' and 'Embarrassment' and of 'Envy' and 'Jealousy', only pick one. So on and so forth.\"},\n",
    "            {\"role\": \"system\", \"content\": \"Stop finding if you've already found 50.\"},\n",
    "            {\"role\": \"system\", \"content\": \"DONT MAKE ANY MISTAKES, check if you did any.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Give me about 50 emotions addressing non-redundant and different human feelings.\"}\n",
    "        ],\n",
    "        response_format=Chr_answer,\n",
    "    )\n",
    "\n",
    "    #you can also proceed from parsed eg json.loads(completion_song.choices[0].message.parsed.json())\n",
    "    out = json.loads(completion.choices[0].message.content)\n",
    "    out = list(out.values())[0]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = get_emotions_list()\n",
    "Characteristic = Enum('Characteristic', dict([(emotion, emotion) for emotion in emotions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EmotionalAssociationScore(BaseModel):\n",
    "    emotion: Characteristic\n",
    "    score: float\n",
    "\n",
    "class EmotionalAssociationScores(BaseModel):\n",
    "    associations: List[EmotionalAssociationScore] = Field(description=\"A list of emotions and associated scores\")\n",
    "\n",
    "def emotional_association_scores(\n",
    "        thing, \n",
    "        emotions = emotions,\n",
    "        min_value = 0,\n",
    "        max_value = 500\n",
    "    ):\n",
    "    prompt = f\"\"\"Assign emotional association scores between {min_value} and {max_value} for the provided thing.\n",
    "            Assign a score for each of the following emotions.\n",
    "            Ensure the scores reflect the association strength for the specified thing.\n",
    "            Thing:\n",
    "            {thing}\n",
    "            \"\"\"\n",
    "            \n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Be a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=EmotionalAssociationScores,\n",
    "    )\n",
    "    #output returns in the defined pydantic style\n",
    "    output = completion.choices[0].message.parsed\n",
    "    return output.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = brands[3]\n",
    "completion = emotional_association_scores(thing)\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tried nested prompt but decided to go with one prompt and a list comprehension\n",
    "[(brand, emotional_association_scores(thing)) for brand in brands[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List 100 best selling American clothing brands \n",
    "client = OpenAI(api_key=k)\n",
    "class Chr_answer(BaseModel):\n",
    "    Characteristics: List[str] = Field(None, description=\"Brands as a list of strings\")\n",
    "\n",
    "#few shot prompt with api\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Be a helpful assistant.\"},\n",
    "        {\"role\": \"system\", \"content\": \"Find 10 non-redundant best selling American clothing brands.\"},\n",
    "        {\"role\": \"system\", \"content\": \"DONT MAKE ANY MISTAKES, check if you did any.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 10 best selling American clothing brands.\"}\n",
    "    ],\n",
    "    response_format=Chr_answer,\n",
    ")\n",
    "\n",
    "#you can also proceed from parsed eg json.loads(completion_song.choices[0].message.parsed.json())\n",
    "out = json.loads(completion.choices[0].message.content)\n",
    "out = list(out.values())[0]\n",
    "brands = out\n",
    "print(brands)\n",
    "print(len(brands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Define the Pydantic model\n",
    "class RelationResponse(BaseModel):\n",
    "    pair: Tuple[str, str]  # First str is object1, second str is object2 linked to the association score\n",
    "    association_score: float  # Association scores for each object2 in relation to object1\n",
    "\n",
    "class RelationResponses(BaseModel):\n",
    "    objects: List[RelationResponse] = Field(None, description=\"List of object emotion pairs and association scores\")\n",
    "\n",
    "# # Define a function to convert the model instance to the desired output structure\n",
    "# def format_output(model_instance: PairAssociation) -> dict:\n",
    "#     # Convert Enum to string\n",
    "#     pair_str = (model_instance.pair[0], model_instance.pair[1].value)\n",
    "#     # Create the final structure\n",
    "#     return {'pair': pair_str, 'association_value': model_instance.association_value}\n",
    "\n",
    "# class RelationResponse(BaseModel):\n",
    "#     object1: str\n",
    "#     associations: Dict[str, float]  # Association scores for each object2 in relation to object1\n",
    "\n",
    "class OpenAIRelationQuantifier:\n",
    "    @staticmethod\n",
    "    def _generate_prompt(object1: str, list2: List[str], min_value: float, max_value: float) -> str:\n",
    "        \"\"\"Generate the prompt for GPT to evaluate the association between one object and a list of other objects. \n",
    "        Minimum and maximum value of the normalized range.\"\"\"\n",
    "        object2_str = ', '.join(list2)\n",
    "        return f\"For the following object '{object1}', assign a score between {min_value} and {max_value} to each object in this list: [{object2_str}]. \" \\\n",
    "               f\"Please return the result as 'Object1 linked to the association score, object2 linked to the association score' pairs, and ensure the scores reflect the association strength.\"\n",
    "\n",
    "    @staticmethod\n",
    "    def quantify_relations(list1: List[str], list2: List[str], min_value: float, max_value: float, model: str, api_key: str) -> List[RelationResponse]:\n",
    "        \"\"\"Quantify the relationships between each object1 and all objects in list2 using GPT.\"\"\"\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        results = []\n",
    "        for object1 in list1:\n",
    "            prompt = OpenAIRelationQuantifier._generate_prompt(object1, list2, min_value, max_value)\n",
    "            print(object1, prompt)\n",
    "            # Send the prompt to GPT using beta.chat.completions.parse method\n",
    "            response = client.beta.chat.completions.parse(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert at analyzing relationships between concepts.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format=RelationResponse\n",
    "            )\n",
    "\n",
    "            # Extract GPT's parsed response\n",
    "            gpt_reply = response.choices[0].message.content\n",
    "            print(gpt_reply)\n",
    "            # score_line = gpt_reply.splitlines()[0]\n",
    "            \n",
    "            # score = float(score_line.split()[-1].rstrip('.'))  # Assumes the score is at the end of the first line\n",
    "\n",
    "            # # Append the result\n",
    "            # results.append(RelationResponse(pair=pair, association_score=score))\n",
    "\n",
    "        return gpt_reply\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "# Quantify the relationships between objects in the two lists\n",
    "relations = OpenAIRelationQuantifier.quantify_relations(\n",
    "    list1=brands[:3], \n",
    "    list2=emotions[:3], \n",
    "    min_value=0, \n",
    "    max_value=9, \n",
    "    model=\"gpt-4o-2024-08-06\", \n",
    "    api_key=k\n",
    "    )\n",
    "\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #as dict\n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import Tuple\n",
    "# class RelationRequest(BaseModel):\n",
    "#     list1: List[str]  # First list of objects (object1)\n",
    "#     list2: List[str]  # Second list of objects (object2)\n",
    "#     model: str = Field(default=\"gpt-4o-2024-08-06\")  # Updated OpenAI model version\n",
    "#     min_value: float = 0  # Minimum value of the normalized range\n",
    "#     max_value: float = 500  # Maximum value of the normalized range (set to 500)\n",
    "\n",
    "# class ObjectPair(BaseModel):\n",
    "#     object1: str\n",
    "#     obj2: str\n",
    "\n",
    "# class RelationResponse(BaseModel):\n",
    "#     pair: ObjectPair\n",
    "#     association_score: float  # The score provided by GPT\n",
    "\n",
    "\n",
    "# class OpenAIRelationQuantifier:\n",
    "#     @staticmethod\n",
    "#     def _generate_prompt(object1: str, object2_list: List[str], min_value: float, max_value: float) -> str:\n",
    "#         \"\"\"Generate the prompt for GPT to evaluate the association between one object and a list of other objects.\"\"\"\n",
    "#         prompt = f\"For the object '{object1}', please provide a score between {min_value} and {max_value} for its association with each of the following objects:\"\n",
    "        \n",
    "#         for idx, obj2 in enumerate(object2_list, 1):\n",
    "#             prompt += f\"\\n{idx}. {obj2}\"\n",
    "#             print(obj2)\n",
    "\n",
    "#         prompt += \"\\n\\nReturn the result as 'object1, obj2' pairs and their associated scores\"\n",
    "\n",
    "#         return prompt\n",
    "\n",
    "#     @staticmethod\n",
    "#     def quantify_relations(request: RelationRequest, api_key: str) -> List[RelationResponse]:\n",
    "#         \"\"\"Quantify the relationships between each object1 and all objects in object2 using GPT.\"\"\"\n",
    "#         openai.api_key = api_key  # Set the API key for OpenAI\n",
    "\n",
    "#         results = []\n",
    "#         for object1 in request.list1:\n",
    "#             prompt = OpenAIRelationQuantifier._generate_prompt(object1, request.list2, request.min_value, request.max_value)\n",
    "\n",
    "#             # Send the prompt to GPT\n",
    "#             response = client.beta.chat.completions.parse(\n",
    "#                 model='gpt-4o-2024-08-06',\n",
    "#                 messages=[\n",
    "#                     {\"role\": \"system\", \"content\": \"You are an expert at analyzing relationships between concepts.\"},\n",
    "#                     {\"role\": \"user\", \"content\": prompt}\n",
    "#                 ],\n",
    "#                 response_format=RelationResponse\n",
    "#             )\n",
    "\n",
    "#             # Extract GPT's parsed response\n",
    "#             results.append(response.choices[0].message.content)\n",
    "#         return results\n",
    "\n",
    "\n",
    "# api_key = k\n",
    "\n",
    "# # Define two lists of objects to compare\n",
    "# list1 = brands[:3]\n",
    "# list2 = emotions[:2]\n",
    "\n",
    "# # Create the relation request object\n",
    "# relation_request = RelationRequest(list1=list1, list2=list2, min_value=0, max_value=500)\n",
    "\n",
    "# # Quantify the relationships between objects in the two lists\n",
    "# relations = OpenAIRelationQuantifier.quantify_relations(relation_request, api_key=api_key)\n",
    "\n",
    "# relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #as joined str in a list\n",
    "# class RelationRequest(BaseModel):\n",
    "#     list1: List[str]  # First list of objects (object1)\n",
    "#     list2: List[str]  # Second list of objects (object2)\n",
    "#     model: str = Field(default=\"gpt-4o-2024-08-06\")  # Updated OpenAI model version\n",
    "#     min_value: float = 0  # Minimum value of the normalized range\n",
    "#     max_value: float = 500  # Maximum value of the normalized range (set to 500)\n",
    "\n",
    "# class ObjectPair(BaseModel):\n",
    "#     object1: str\n",
    "#     obj2: str\n",
    "\n",
    "# class RelationResponse(BaseModel):\n",
    "#     pair: ObjectPair\n",
    "#     association_score: float  # The score provided by GPT\n",
    "\n",
    "# class OpenAIRelationQuantifier:\n",
    "#     @staticmethod\n",
    "#     def _generate_prompt(object1: str, object2_list: List[str], min_value: float, max_value: float) -> str:\n",
    "#         \"\"\"Generate the prompt for GPT to evaluate the association between one object and a list of other objects.\"\"\"\n",
    "#         object2_str = ', '.join(object2_list)\n",
    "#         return f\"For the following object '{object1}', assign a score between {min_value} and {max_value} to each object in this list: [{object2_str}]. \" \\\n",
    "#                f\"Please return the result as 'object1 obj2' pairs, and ensure the scores reflect the association strength.\"\n",
    "\n",
    "#     @staticmethod\n",
    "#     def quantify_relations(request: RelationRequest, api_key: str) -> List[RelationResponse]:\n",
    "#         \"\"\"Quantify the relationships between each object1 and all objects in object2 using GPT.\"\"\"\n",
    "#         openai.api_key = api_key  # Set the API key for OpenAI\n",
    "\n",
    "#         results = []\n",
    "#         for object1 in request.list1:\n",
    "#             prompt = OpenAIRelationQuantifier._generate_prompt(object1, request.list2, request.min_value, request.max_value)\n",
    "\n",
    "#             response = client.beta.chat.completions.parse(\n",
    "#                 model=request.model,\n",
    "#                 messages=[\n",
    "#                     {\"role\": \"system\", \"content\": \"You are an expert at analyzing relationships between concepts.\"},\n",
    "#                     {\"role\": \"user\", \"content\": prompt}\n",
    "#                 ],\n",
    "#                 response_format=RelationResponse\n",
    "#             )\n",
    "            \n",
    "#             results.append(relations.choices[0].message.content)\n",
    "\n",
    "#         return results\n",
    "\n",
    "\n",
    "# api_key = k\n",
    "\n",
    "# # Define two lists of objects to compare\n",
    "# list1 = [\"Nike\", \"lululemon\"]\n",
    "# list2 = [\"Joy\",\"liberated\"]\n",
    "\n",
    "# # Create the relation request object\n",
    "# relation_request = RelationRequest(list1=list1, list2=list2, min_value=0, max_value=500)\n",
    "\n",
    "# # Quantify the relationships between objects in the two lists\n",
    "# relations = OpenAIRelationQuantifier.quantify_relations(relation_request, api_key=api_key)\n",
    "# relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method\n",
    "#embedding dimension is emotions\n",
    "#talk about options\n",
    "#get the brands, go through 50 emotins at a time\n",
    "#cosine: normalize first: l2 norm = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Goal: Get binary association of 100 brands and 1 book with 50 emotions. Multibinarize and do a correlation. in the example, no correlation with any of the brands\n",
    "# # create a list of 50 human emotions\n",
    "# client = OpenAI(api_key=k)\n",
    "# class Chr_answer(BaseModel):\n",
    "#     Characteristics: List[str] = Field(None, description=\"Emotions as a list of strings\")\n",
    "\n",
    "# #few shot prompt with api\n",
    "# completion = client.beta.chat.completions.parse(\n",
    "#     model=\"gpt-4o-2024-08-06\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"Be a helpful assistant.\"},\n",
    "#         {\"role\": \"system\", \"content\": \"Find 50 non-redundant human emotions. These emotions should be different from one another, meaning that for example, of the emotions 'joy' and 'happiness', only pick one. Of 'Shame' and 'Embarrassment' and of 'Envy' and 'Jealousy', only pick one. So on and so forth.\"},\n",
    "#         {\"role\": \"system\", \"content\": \"Stop finding if you've already found 50.\"},\n",
    "#         {\"role\": \"system\", \"content\": \"DONT MAKE ANY MISTAKES, check if you did any.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Give me about 50 emotions addressing non-redundant and different human feelings.\"}\n",
    "#     ],\n",
    "#     response_format=Chr_answer,\n",
    "# )\n",
    "\n",
    "# #you can also proceed from parsed eg json.loads(completion_song.choices[0].message.parsed.json())\n",
    "# out = json.loads(completion.choices[0].message.content)\n",
    "# out = list(out.values())[0]\n",
    "# emotions = out\n",
    "# print(emotions)\n",
    "# print(len(emotions))\n",
    "\n",
    "# #List 100 best selling American clothing brands and, for each brand, list which one of the emotions in the given list are most closely associated with (Either associated or not, binary).\"\n",
    "# client = OpenAI(api_key=k)\n",
    "\n",
    "# Characteristic = Enum('Characteristic', dict([(i, i) for i in emotions])) #MyEnumType = Enum('MyEnumType', myEnumStrings)\n",
    "\n",
    "# class BrandAttributes(BaseModel):\n",
    "#     brand_name: str = Field(None, description=\" name as str\")\n",
    "#     characteristics: List[Characteristic] = Field(None, description=\" list of characteristics as str\")\n",
    "\n",
    "# class BrandChars(BaseModel):\n",
    "#     a: List[BrandAttributes] = Field(None, description=\"List of BrandAttributes\")\n",
    "    \n",
    "# completion = client.beta.chat.completions.parse(\n",
    "#     model=\"gpt-4o-2024-08-06\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"Be a helpful assistant.\"},\n",
    "#         {\"role\": \"system\", \"content\": \"List 100 best selling American clothing brands and, for each brand, list which one of the emotions in the given list are most closely associated with.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"List 100 best selling American clothing brands and, for each brand, list which one of the emotions in the given list are most closely associated with.\"}\n",
    "#     ],\n",
    "#     response_format=BrandChars,\n",
    "# )\n",
    "\n",
    "# out = json.loads(completion.choices[0].message.content)\n",
    "# brand_emotions = out\n",
    "# out = list(out.values())[0]\n",
    "# df_brand = pd.DataFrame(data = [i.values() for i in out], columns = ['Name','characteristic'])\n",
    "# df_brand\n",
    "\n",
    "# #Take the example song, and list which one of the emotions in the given list are most closely associated with the song.\")\n",
    "\n",
    "# client = OpenAI(api_key=k)\n",
    "\n",
    "# Characteristic = Enum('Characteristic', dict([(i, i) for i in emotions])) \n",
    "\n",
    "# class BookAttributes(BaseModel):\n",
    "#     book_name: str\n",
    "#     book_writer :str = Field(None, description=\"writer as str\")\n",
    "#     characteristics: List[Characteristic]\n",
    "\n",
    "# class BooksChars(BaseModel):\n",
    "#     a: List[BookAttributes] = Field(None, description=\"List of book attributes.\")\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# As prompt, take the book 'Summer Island', list its writer, and a list of the emotions from the given list that are most closely associated with the book.\n",
    "# \"\"\"\n",
    "\n",
    "# completion = client.beta.chat.completions.parse(\n",
    "#     model=\"gpt-4o-2024-08-06\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"Be a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ],\n",
    "#     response_format=BooksChars,\n",
    "# )\n",
    "# out = json.loads(completion.choices[0].message.content)\n",
    "# out = list(out.values())[0]\n",
    "# df_book = pd.DataFrame(data = [i.values() for i in out], columns = ['Name','Writer', 'characteristic'])\n",
    "# df_book\n",
    "\n",
    "# #MultilabelBinarize brands and song\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# mlb = MultiLabelBinarizer(classes=emotions)\n",
    "\n",
    "# df_brand2 = pd.DataFrame(mlb.fit_transform(df_brand['characteristic']), columns = emotions)\n",
    "# df_brand_mb = pd.merge(df_brand2, df_brand[['Name']], how ='left', left_index=True, right_index=True)\n",
    "# df_brand_mb.head()\n",
    "\n",
    "# mlb = MultiLabelBinarizer(classes=emotions)\n",
    "\n",
    "# df_book2 = pd.DataFrame(mlb.fit_transform(df_book['characteristic']), columns = emotions)\n",
    "# df_book_mb = pd.merge(df_book2, df_book[['Name']], how ='left', left_index=True, right_index=True)\n",
    "# df_book_mb.head()\n",
    "\n",
    "# df_b_T = df_brand_mb.set_index('Name').T\n",
    "# df_bo_T = df_book_mb.set_index('Name').T\n",
    "\n",
    "# df_bo_T\n",
    "\n",
    "\n",
    "# df_b_T.corrwith(df_bo_T, axis = 1)\n",
    "\n",
    "\n",
    "#---------------\n",
    "#2nd method, I tried, was to go through every brand from 10, and every emotion from 50, for every pair assign an association value (0 to 500):\n",
    "\n",
    "# # Prompt GPT to determine the association between brand and emotion pairs based on semantic or contextual understanding.\n",
    "  \n",
    "# # Define the Pydantic models for the input and output\n",
    "# class ObjectPair(BaseModel):\n",
    "#     object1: str\n",
    "#     object2: str\n",
    "\n",
    "# class RelationRequest(BaseModel):\n",
    "#     list1: List[str]  # First list of objects\n",
    "#     list2: List[str]  # Second list of objects\n",
    "#     model: str = Field(default=\"gpt-4o-2024-08-06\")  # OpenAI model version\n",
    "#     min_value: float = 0  # Minimum value of the normalized range\n",
    "#     max_value: float = 500  # Maximum value of the normalized range \n",
    "\n",
    "# class RelationResponse(BaseModel):\n",
    "#     pair: ObjectPair\n",
    "#     association_score: float  # The score provided by GPT\n",
    "\n",
    "# class OpenAIRelationQuantifier:\n",
    "#     @staticmethod\n",
    "#     def _generate_prompt(pair: ObjectPair, min_value: float, max_value: float) -> str:\n",
    "#         \"\"\"Generate the prompt for GPT to evaluate the association between two objects.\"\"\"\n",
    "#         return f\"On a scale from {min_value} to {max_value}, how strongly are the following two items related?\\n\\n\" \\\n",
    "#                f\"Item 1: {pair.object1}\\nItem 2: {pair.object2}\\n\" \\\n",
    "#                f\"Please provide a score and a short explanation of their relationship.\"\n",
    "\n",
    "#     @staticmethod\n",
    "#     def quantify_relations(request: RelationRequest) -> List[RelationResponse]:\n",
    "#         \"\"\"Quantify the relationships between each pair of objects using GPT.\"\"\"\n",
    "#         client = OpenAI(api_key=k)  \n",
    "\n",
    "#         results = []\n",
    "#         for object1 in request.list1:\n",
    "#             for object2 in request.list2:\n",
    "#                 pair = ObjectPair(object1=object1, object2=object2)\n",
    "#                 prompt = OpenAIRelationQuantifier._generate_prompt(pair, request.min_value, request.max_value)\n",
    "\n",
    "#                 # Send the prompt to GPT using beta.chat.completions.parse method\n",
    "#                 response = client.beta.chat.completions.parse(\n",
    "#                     model=request.model,\n",
    "#                     messages=[\n",
    "#                         {\"role\": \"system\", \"content\": \"You are an expert at analyzing relationships between concepts.\"},\n",
    "#                         {\"role\": \"user\", \"content\": prompt}\n",
    "#                     ]\n",
    "#                 )\n",
    "\n",
    "#                 # Extract GPT's parsed response\n",
    "#                 gpt_reply = response.choices[0].message.content\n",
    "#                 score_line = gpt_reply.splitlines()[0]\n",
    "#                 score = float(score_line.split()[-1].rstrip('.'))  # Assumes the score is at the end of the first line\n",
    "\n",
    "#                 # Append the result\n",
    "#                 results.append(RelationResponse(pair=pair, association_score=score))\n",
    "\n",
    "#         return results\n",
    "\n",
    "# # Usage\n",
    "\n",
    "# # Define two lists of objects to compare\n",
    "# list1 = brands[:3]\n",
    "# list2 = emotions[:5]\n",
    "\n",
    "# # Create the relation request object\n",
    "# relation_request = RelationRequest(list1=list1, list2=list2, min_value=0, max_value=500)\n",
    "\n",
    "# # Quantify the relationships between objects in the two lists\n",
    "# relations = OpenAIRelationQuantifier.quantify_relations(relation_request)\n",
    "\n",
    "# # Display the results\n",
    "# for relation in relations:\n",
    "#     print(f\"Pair: ({relation.pair.object1}, {relation.pair.object2}) - Association Score: {relation.association_score:.2f}\")\n",
    "# relations\n",
    "\n",
    "\n",
    "# df=pd.DataFrame()\n",
    "# df['brand']= [i.pair.object1 for i in relations]\n",
    "# df['emotion']= [i.pair.object2 for i in relations]\n",
    "# df['score']= [i.association_score for i in relations]\n",
    "\n",
    "# print(df)\n",
    "# df = df.pivot(index='brand',columns ='emotion', values='score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
